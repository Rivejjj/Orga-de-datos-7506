{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Organizaci&oacute;n de Datos</center>\n",
    "#### <center>C&aacute;tedra Ing. Rodriguez, Juan Manuel </center>\n",
    "\n",
    "## <center>Trabajo Práctico 2 : Propiedades en Venta</center>\n",
    "## <center>Parte 3</center>\n",
    "### <center> Grupo 22 </center>\n",
    "### <center> Integrantes: </center>\n",
    "##### <center> Federico Martin Forte,  Francisco Sobral,  Ian Klaus von der Heyde,  Juan Pablo Aschieri,  Joaquin Rivero  </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carga de Datos y librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "viviendas_train=pd.read_csv('./TP1/Datasets/viviendas_caba_train.csv').copy()\n",
    "viviendas_test=pd.read_csv('./TP1/Datasets/viviendas_caba_test.csv').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "viviendas = pd.concat([viviendas_train, viviendas_test]).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Ensamble de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo los datos de entrenamiento y prueba de nuestro Random Forest\n",
    "\n",
    "y_train = pd.read_csv('./TP1/Datasets/y_train_property_type.csv').copy()\n",
    "y_test = pd.read_csv('./TP1/Datasets/y_test_property_type.csv').copy()\n",
    "\n",
    "x_train = pd.read_csv('./TP1/Datasets/x_train_property_type.csv').copy()\n",
    "x_test = pd.read_csv('./TP1/Datasets/x_test_property_type.csv').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting sobre modelos nuevos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a evaluar distintos clasificadores para ver cual me conviene para obtener mejores metricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predicciones con decision tree classifier\n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc = dtc.fit(x_train, y_train)\n",
    "y_pred_dtc = dtc.predict(x_test)\n",
    "\n",
    "\n",
    "# Predicciones con logistic regression model\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(x_train, y_train)\n",
    "y_pred_lr = lr.predict(x_test)\n",
    "\n",
    "\n",
    "# Predicciones con naive bayes model\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(x_train, y_train)\n",
    "y_pred_gnb = gnb.predict(x_test)\n",
    "\n",
    "# Predicciones con modelo AdaBoost\n",
    "ada_boost_clf = AdaBoostClassifier()\n",
    "ada_boost_clf.fit(x_train, y_train)\n",
    "y_pred_ada = ada_boost_clf.predict(x_test)\n",
    "\n",
    "# Predicciones con modelo Random Forest\n",
    "rnd_clf = RandomForestClassifier()\n",
    "rnd_clf.fit(x_train, y_train)\n",
    "y_pred_rnd = rnd_clf.predict(x_test)\n",
    "\n",
    "# Predicciones con modelo KNN\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(x_train, y_train)\n",
    "y_pred_knn = knn_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimo los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score of decision tree classifier: 0.7051\n",
      "F1-score of support vector machine: 0.5452\n",
      "F1-score of naive bayes classifier: 0.3733\n",
      "F1-score of random forest classifier: 0.744\n",
      "F1-score of KNN classifier: 0.6116\n",
      "F1-score of AdaBoost classifier: 0.5932\n"
     ]
    }
   ],
   "source": [
    "# evaluating the models based on their f-1 scores\n",
    "f1_dtc = f1_score(y_test, y_pred_dtc,average='weighted')\n",
    "f1_lr = f1_score(y_test, y_pred_lr,average='weighted')\n",
    "f1_gnb = f1_score(y_test, y_pred_gnb,average='weighted')\n",
    "f1_rnd = f1_score(y_test, y_pred_rnd,average='weighted')\n",
    "f1_knn = f1_score(y_test, y_pred_knn,average='weighted')\n",
    "f1_ada = f1_score(y_test, y_pred_ada,average='weighted')\n",
    "\n",
    "# print the f-1 scores\n",
    "print('F1-score of decision tree classifier: {}'.format(np.round(f1_dtc,4)))\n",
    "print('F1-score of support vector machine: {}'.format(np.round(f1_lr,4)))\n",
    "print('F1-score of naive bayes classifier: {}'.format(np.round(f1_gnb,4)))\n",
    "print('F1-score of random forest classifier: {}'.format(np.round(f1_rnd,4)))\n",
    "print('F1-score of KNN classifier: {}'.format(np.round(f1_knn,4)))\n",
    "print('F1-score of AdaBoost classifier: {}'.format(np.round(f1_ada,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aca podemos ver que los mejores 3 modelos fueron los de Decision Tree Classifier, Random Forest Classifier y KNN. Procedo a realizar el Voting con dichos tres modelos. Para obtener mejores resultados, tambien voy a evaluar realizando una votacion de tipo Hard y una de tipo Soft y utilizar la que mejores resultados de."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo ensemble de Votación Hard\n",
    "vot_clf_h = VotingClassifier(estimators = [('dtc', dtc), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'hard')\n",
    "\n",
    "#Creo ensemble de Votación Soft\n",
    "vot_clf_s = VotingClassifier(estimators = [('dtc', dtc), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'soft')\n",
    "\n",
    "\n",
    "#Entreno ambos ensembles\n",
    "vot_clf_h.fit(x_train, y_train)\n",
    "vot_clf_s.fit(x_train, y_train)\n",
    "\n",
    "#Evaluo en conjunto de test\n",
    "pred_h = vot_clf_h.predict(x_test)\n",
    "pred_s = vot_clf_s.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS HARD VOTING:\n",
      "\n",
      "Accuracy Score: 0.7319659776055125\n",
      "F1-score: 0.7315100896527955\n",
      "Precision Score :  0.7325108854056845\n",
      "Recall Score : 0.7319659776055125\n",
      "\n",
      "RESULTADOS SOFT VOTING:\n",
      "\n",
      "Accuracy Score: 0.7263673557278209\n",
      "F1-score: 0.7258304921129773\n",
      "Precision Score:  0.7265342829423058\n",
      "Recall Score: 0.7263673557278209\n"
     ]
    }
   ],
   "source": [
    "#Calculo accuracy\n",
    "print(\"RESULTADOS HARD VOTING:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, pred_h)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, pred_h,average=\"weighted\")))\n",
    "print(\"Precision Score : \",precision_score(y_test,pred_h,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score :\" , recall_score(y_test, pred_h, pos_label='positive',average=\"weighted\"))\n",
    "\n",
    "print(\"\\nRESULTADOS SOFT VOTING:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, pred_s)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, pred_s,average=\"weighted\")))\n",
    "print(\"Precision Score: \",precision_score(y_test,pred_s,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score:\" , recall_score(y_test, pred_s, pos_label='positive',average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluimos que conviene utilizar el metodo de Hard voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting sobre nuestros modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo los modelos del TP1\n",
    "\n",
    "knn_clf = joblib.load('./TP1/Models/knn_mejor_performance')\n",
    "rnd_clf = joblib.load('./TP1/Models/rf_mejor_performance')\n",
    "dtc_clf = joblib.load('./TP1/Models/arbol_mejor_performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones con modelo Knn\n",
    "y_pred_knn = knn_clf.predict(x_test)\n",
    "\n",
    "# Predicciones con modelo Random Forest\n",
    "y_pred_rnd = rnd_clf.predict(x_test)\n",
    "\n",
    "# Predicciones con modelo DTC\n",
    "y_pred_dtc = dtc_clf.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTADOS K_nearest neighbors:\n",
      "\n",
      "Accuracy Score: 0.6763027562446167\n",
      "F1-score: 0.674845002332972\n",
      "Precision Score:  0.6760850373211389\n",
      "Recall Score: 0.6763027562446167\n",
      "\n",
      "RESULTADOS Random Forest:\n",
      "\n",
      "Accuracy Score: 0.743270887166236\n",
      "F1-score: 0.7422169581799882\n",
      "Precision Score:  0.7453531747293577\n",
      "Recall Score: 0.743270887166236\n",
      "\n",
      "RESULTADOS Decision Tree Classifier:\n",
      "\n",
      "Accuracy Score: 0.7037575366063739\n",
      "F1-score: 0.7038775368622544\n",
      "Precision Score:  0.7041015747256649\n",
      "Recall Score: 0.7037575366063739\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRESULTADOS K_nearest neighbors:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred_knn)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, y_pred_knn,average=\"weighted\")))\n",
    "print(\"Precision Score: \",precision_score(y_test,y_pred_knn,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score:\" , recall_score(y_test, y_pred_knn, pos_label='positive',average=\"weighted\"))\n",
    "\n",
    "print(\"\\nRESULTADOS Random Forest:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred_rnd)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, y_pred_rnd,average=\"weighted\")))\n",
    "print(\"Precision Score: \",precision_score(y_test,y_pred_rnd,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score:\" , recall_score(y_test, y_pred_rnd, pos_label='positive',average=\"weighted\"))\n",
    "\n",
    "print(\"\\nRESULTADOS Decision Tree Classifier:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, y_pred_dtc)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, y_pred_dtc,average=\"weighted\")))\n",
    "print(\"Precision Score: \",precision_score(y_test,y_pred_dtc,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score:\" , recall_score(y_test, y_pred_dtc, pos_label='positive',average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras predecir los modelos individualmente, vamos a realizar el Voting para ver si mejora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTADOS HARD VOTING:\n",
      "\n",
      "Accuracy Score: 0.7337962962962963\n",
      "F1-score: 0.7331204066801106\n",
      "Precision Score :  0.7346815391566508\n",
      "Recall Score : 0.7337962962962963\n",
      "\n",
      "RESULTADOS SOFT VOTING:\n",
      "\n",
      "Accuracy Score: 0.7256675279931094\n",
      "F1-score: 0.7251570618717043\n",
      "Precision Score:  0.7258290336205333\n",
      "Recall Score: 0.7256675279931094\n"
     ]
    }
   ],
   "source": [
    "#Creo ensemble de Votación Hard\n",
    "vot_clf_h = VotingClassifier(estimators = [('dtc', dtc_clf), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'hard')\n",
    "\n",
    "#Creo ensemble de Votación Soft\n",
    "vot_clf_s = VotingClassifier(estimators = [('dtc', dtc_clf), ('rnd', rnd_clf), ('knn', knn_clf)], voting = 'soft')\n",
    "\n",
    "\n",
    "#Entreno ambos ensembles\n",
    "vot_clf_h.fit(x_train, y_train)\n",
    "vot_clf_s.fit(x_train, y_train)\n",
    "\n",
    "#Evaluo en conjunto de test\n",
    "pred_h = vot_clf_h.predict(x_test)\n",
    "pred_s = vot_clf_s.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "#Calculo accuracy\n",
    "print(\"RESULTADOS HARD VOTING:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, pred_h)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, pred_h,average=\"weighted\")))\n",
    "print(\"Precision Score : \",precision_score(y_test,pred_h,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score :\" , recall_score(y_test, pred_h, pos_label='positive',average=\"weighted\"))\n",
    "\n",
    "print(\"\\nRESULTADOS SOFT VOTING:\\n\")\n",
    "print('Accuracy Score: {}'.format(accuracy_score(y_test, pred_s)))\n",
    "print('F1-score: {}'.format(f1_score(y_test, pred_s,average=\"weighted\")))\n",
    "print(\"Precision Score: \",precision_score(y_test,pred_s,pos_label='positive',average=\"weighted\"))\n",
    "print(\"Recall Score:\" , recall_score(y_test, pred_s, pos_label='positive',average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluimos que conviene utilizar el metodo de Hard voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones Voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver tras comparar los dos modelos, nos da una metrica superior el Voting realizado con los modelos del TP1. Esto se debe a que dichos modelos fueron ajustados de mejor manera y son mas precisos. Sin embargo, algo que nos llamo la atención es que los modelos de Random Forest obtienen mejores resultados que el propio Voting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import joblib\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecciono variables regresoras y a predecir\n",
    "x_train = viviendas_train[['latitud','longitud', 'property_surface_total', 'property_surface_covered']]\n",
    "x_test = viviendas_test[['latitud','longitud', 'property_surface_total', 'property_surface_covered']]\n",
    "\n",
    "y_train = viviendas_train.property_price\n",
    "y_test = viviendas_test.property_price\n",
    "\n",
    "#Normalizo\n",
    "scaler = MinMaxScaler() \n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos Stacking con modelos nuevos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo los modelos\n",
    "knn_n = KNeighborsRegressor()\n",
    "gbr_n = GradientBoostingRegressor()\n",
    "xgboost_n = xgb.XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    " \n",
    " # define the base models\n",
    " level0 = list()\n",
    " level0.append(('knn', knn_n)) \n",
    " level0.append(('xgboost', xgboost_n)) \n",
    " level0.append(('gbr', gbr_n)) \n",
    "\n",
    " # define meta learner model\n",
    " level1 = LinearRegression()\n",
    "\n",
    " # define the stacking ensemble\n",
    " model = StackingRegressor(estimators=level0, \n",
    "                                    final_estimator=level1, \n",
    "                                    passthrough=True, \n",
    "                                    cv=5,\n",
    "                                    verbose=2)\n",
    " \n",
    "\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "\n",
    "def get_models():\n",
    " models = dict()\n",
    " models['knn'] = knn_n\n",
    " models['xgb'] = xgboost_n\n",
    " models['gbr'] = gbr_n\n",
    " models['stacking'] = get_stacking()\n",
    "\n",
    " return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para plotear a performance\n",
    "def plot_results(model_scores, name):\n",
    "    \n",
    "    model_names = list(model_scores.keys())\n",
    "    results = [model_scores[model] for model in model_names]\n",
    "    fig = go.Figure()\n",
    "    for model, result in zip(model_names, results):\n",
    "        fig.add_trace(go.Box(\n",
    "            y=result,\n",
    "            name=model,\n",
    "            boxpoints='all',\n",
    "            jitter=0.5,\n",
    "            whiskerwidth=0.2,\n",
    "            marker_size=2,\n",
    "            line_width=1)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "    title='Performance of Different Models Using 5-Fold Cross-Validation',\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Accuracy',\n",
    "    showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_metricas_regresion(target_test, precios_predichos):\n",
    "    # Mean Square Error\n",
    "    mse = metrics.mean_squared_error(\n",
    "        y_true=target_test, y_pred=precios_predichos, squared=True\n",
    "    )\n",
    "\n",
    "    print(f\"El error (mse) de test es: {mse}\")\n",
    "\n",
    "    # Root Mean Square Error\n",
    "    rmse = metrics.mean_squared_error(\n",
    "        y_true=target_test, y_pred=precios_predichos, squared=False\n",
    "    )\n",
    "\n",
    "    print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n",
    "    r2 = metrics.r2_score(target_test, precios_predichos)\n",
    "    print(f\"El score R2 es: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking = get_stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    5.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   18.4s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(cv=5,\n",
       "                  estimators=[(&#x27;knn&#x27;, KNeighborsRegressor()),\n",
       "                              (&#x27;xgboost&#x27;,\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            gpu_id=None, grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            i...\n",
       "                                            learning_rate=None, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            predictor=None, random_state=None, ...)),\n",
       "                              (&#x27;gbr&#x27;, GradientBoostingRegressor())],\n",
       "                  final_estimator=LinearRegression(), passthrough=True,\n",
       "                  verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(cv=5,\n",
       "                  estimators=[(&#x27;knn&#x27;, KNeighborsRegressor()),\n",
       "                              (&#x27;xgboost&#x27;,\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            gpu_id=None, grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            i...\n",
       "                                            learning_rate=None, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            predictor=None, random_state=None, ...)),\n",
       "                              (&#x27;gbr&#x27;, GradientBoostingRegressor())],\n",
       "                  final_estimator=LinearRegression(), passthrough=True,\n",
       "                  verbose=2)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knn</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgboost</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(cv=5,\n",
       "                  estimators=[('knn', KNeighborsRegressor()),\n",
       "                              ('xgboost',\n",
       "                               XGBRegressor(base_score=None, booster=None,\n",
       "                                            callbacks=None,\n",
       "                                            colsample_bylevel=None,\n",
       "                                            colsample_bynode=None,\n",
       "                                            colsample_bytree=None,\n",
       "                                            early_stopping_rounds=None,\n",
       "                                            enable_categorical=False,\n",
       "                                            eval_metric=None,\n",
       "                                            feature_types=None, gamma=None,\n",
       "                                            gpu_id=None, grow_policy=None,\n",
       "                                            importance_type=None,\n",
       "                                            i...\n",
       "                                            learning_rate=None, max_bin=None,\n",
       "                                            max_cat_threshold=None,\n",
       "                                            max_cat_to_onehot=None,\n",
       "                                            max_delta_step=None, max_depth=None,\n",
       "                                            max_leaves=None,\n",
       "                                            min_child_weight=None, missing=nan,\n",
       "                                            monotone_constraints=None,\n",
       "                                            n_estimators=100, n_jobs=None,\n",
       "                                            num_parallel_tree=None,\n",
       "                                            predictor=None, random_state=None, ...)),\n",
       "                              ('gbr', GradientBoostingRegressor())],\n",
       "                  final_estimator=LinearRegression(), passthrough=True,\n",
       "                  verbose=2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacking.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12:38:06] WARNING: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0ac76685cf763591d-1/xgboost/xgboost-ci-windows/src/learner.cc:553: \n",
      "  If you are loading a serialized model (like pickle in Python, RDS in R) generated by\n",
      "  older XGBoost, please export the model by calling `Booster.save_model` from that version\n",
      "  first, then load it back in current version. See:\n",
      "\n",
      "    https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
      "\n",
      "  for more details about differences between saving model and serializing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "stacking.fit(x_train, y_train)\n",
    "joblib.dump(stacking, './Models/stacking_modelos_nuevos')\"\"\"\n",
    "\n",
    "stacking = joblib.load('./Models/stacking_modelos_nuevos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Regressor:\n",
      "El error (mse) de test es: 11821616533.516012\n",
      "El error (rmse) de test es: 108727.25754619221\n",
      "El score R2 es: 0.873594814426625\n",
      "\n",
      "XGBoost Regressor:\n",
      "El error (mse) de test es: 10919589772.080612\n",
      "El error (rmse) de test es: 104496.84096699102\n",
      "El score R2 es: 0.8832399302065291\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "El error (mse) de test es: 14190981610.85262\n",
      "El error (rmse) de test es: 119125.90654787321\n",
      "El score R2 es: 0.848259867091573\n"
     ]
    }
   ],
   "source": [
    "#Evaluo las metricas de los modelos por individual\n",
    "\n",
    "print(\"\\nKNN Regressor:\")\n",
    "knn_f = knn_n.fit(x_train, y_train)\n",
    "y_pred = knn_f.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)\n",
    "\n",
    "print(\"\\nXGBoost Regressor:\")\n",
    "xgboost_f = xgboost_n.fit(x_train, y_train)\n",
    "y_pred = xgboost_f.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)\n",
    "\n",
    "print(\"\\nGradient Boosting Regressor:\")\n",
    "gbr_f = gbr_n.fit(x_train, y_train)\n",
    "y_pred = gbr_f.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error (mse) de test es: 9946147391.381865\n",
      "El error (rmse) de test es: 99730.37346456628\n",
      "El score R2 es: 0.8936486729049878\n"
     ]
    }
   ],
   "source": [
    "#Evaluo el Stacking de los modelos\n",
    "\n",
    "y_pred = stacking.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como las metricas por separado son inferiores al Stacking. Esto nos demuestra que nuestro modelo esta bien hecho y conviene utilizar el Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizamos Stacking con nuestros modelos anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo los modelos del TP1\n",
    "\n",
    "gbr = joblib.load('./TP1/Models/rand_GBR')\n",
    "knn = joblib.load('./TP1/Models/rand_knn')\n",
    "xgboost = joblib.load('./TP1/Models/rand_xgb2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a stacking ensemble of models\n",
    "def get_stacking():\n",
    " \n",
    " # define the base models\n",
    " level0 = list()\n",
    " level0.append(('knn', knn )) \n",
    " level0.append(('xgboost', xgboost )) \n",
    " level0.append(('gbr', gbr)) \n",
    "\n",
    " # define meta learner model\n",
    " level1 = LinearRegression()\n",
    "\n",
    " # define the stacking ensemble\n",
    " model = StackingRegressor(estimators=level0, \n",
    "                                    final_estimator=level1, \n",
    "                                    passthrough=True, \n",
    "                                    cv=5,\n",
    "                                    verbose=2)\n",
    " \n",
    "\n",
    " return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of models to evaluate\n",
    "\n",
    "def get_models():\n",
    " models = dict()\n",
    " models['knn'] = knn \n",
    " models['xgb'] = xgboost \n",
    " models['gbr'] = gbr \n",
    " models['stacking'] = get_stacking()\n",
    "\n",
    " return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\tcv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "\tscores = cross_val_score(model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise')\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion para plotear a performance\n",
    "def plot_results(model_scores, name):\n",
    "    \n",
    "    model_names = list(model_scores.keys())\n",
    "    results = [model_scores[model] for model in model_names]\n",
    "    fig = go.Figure()\n",
    "    for model, result in zip(model_names, results):\n",
    "        fig.add_trace(go.Box(\n",
    "            y=result,\n",
    "            name=model,\n",
    "            boxpoints='all',\n",
    "            jitter=0.5,\n",
    "            whiskerwidth=0.2,\n",
    "            marker_size=2,\n",
    "            line_width=1)\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "    title='Performance of Different Models Using 5-Fold Cross-Validation',\n",
    "    paper_bgcolor='rgb(243, 243, 243)',\n",
    "    plot_bgcolor='rgb(243, 243, 243)',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Accuracy',\n",
    "    showlegend=False)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imprimir_metricas_regresion(target_test, precios_predichos):\n",
    "    # Mean Square Error\n",
    "    mse = metrics.mean_squared_error(\n",
    "        y_true=target_test, y_pred=precios_predichos, squared=True\n",
    "    )\n",
    "\n",
    "    print(f\"El error (mse) de test es: {mse}\")\n",
    "\n",
    "    # Root Mean Square Error\n",
    "    rmse = metrics.mean_squared_error(\n",
    "        y_true=target_test, y_pred=precios_predichos, squared=False\n",
    "    )\n",
    "\n",
    "    print(f\"El error (rmse) de test es: {rmse}\")\n",
    "\n",
    "    r2 = metrics.r2_score(target_test, precios_predichos)\n",
    "    print(f\"El score R2 es: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking = get_stacking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "stacking.fit(x_train, y_train)\n",
    "joblib.dump(stacking, './Models/stacking_modelos_tp1')\n",
    "\"\"\"\n",
    "stacking = joblib.load('./Models/stacking_modelos_tp1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN Regressor:\n",
      "El error (mse) de test es: 9903379174.372705\n",
      "El error (rmse) de test es: 99515.72325202035\n",
      "El score R2 es: 0.894105981293596\n",
      "\n",
      "XGBoost Regressor:\n",
      "El error (mse) de test es: 9683927892.144327\n",
      "El error (rmse) de test es: 98406.95042599546\n",
      "El score R2 es: 0.8964525114805418\n",
      "\n",
      "Gradient Boosting Regressor:\n",
      "El error (mse) de test es: 8607454252.030666\n",
      "El error (rmse) de test es: 92776.36688311666\n",
      "El score R2 es: 0.9079629381516854\n"
     ]
    }
   ],
   "source": [
    "#Evaluo las metricas de los modelos por individual\n",
    "\n",
    "print(\"\\nKNN Regressor:\")\n",
    "y_pred = knn.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)\n",
    "\n",
    "print(\"\\nXGBoost Regressor:\")\n",
    "y_pred = xgboost.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)\n",
    "\n",
    "y_pred = gbr.predict(x_test)\n",
    "print(\"\\nGradient Boosting Regressor:\")\n",
    "imprimir_metricas_regresion(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error (mse) de test es: 8398373997.479234\n",
      "El error (rmse) de test es: 91642.64289881231\n",
      "El score R2 es: 0.910198573887405\n"
     ]
    }
   ],
   "source": [
    "#Evaluo el Stacking de los modelos\n",
    "\n",
    "y_pred = stacking.predict(x_test)\n",
    "imprimir_metricas_regresion(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar como las metricas por separado son inferiores al Stacking. Esto nos demuestra que nuestro modelo esta bien hecho y conviene utilizar el Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en los dos modelos planteados, obtenemos que el segundo modelo es el mas preciso. Esto es lo mas lógico ya que es un modelo el cual utiliza modelos de regresión creados especificamente en el TP1, mientras que el otro son modelos creados de cero sin ninguna especificación."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspectos relevantes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Parte 1: Procesamiento de Lenguaje Natural\n",
    "  + Para elegir las nuevas columnas buscamos las palabras con más apariciones ordenadas de mayor a menor, y luego elegimos algunas de estas palabras que creímos que aportaban valor.\n",
    "  + La ampliación del dataset contribuyó a mejorar el resultado de XGBoost pero al volver a buscar \"mejores hiperparametros\" llegamos prácticamente a lo mismo, ya que creemos que las nuevas columnas no agregan suficiente informacion como para conseguir mejores métricas con distintos hiperparametros. \n",
    "  + Dado las mejoras en los resultados de XGBoost concluimos que nuestras nuevas columnas sí aportan valor al dataset original, lo que es lógico por su naturaleza. Por ejemplo la columna \"a_estrenar\" aporta mucho valor, ya que no es lo mismo una vivienda nueva que usada.  \n",
    "<br />\n",
    "    \n",
    "+ Parte 2: Redes Neuronales  \n",
    "  + Corrimos muchos modelos de redes con diferentes combinaciones de hiperparametros sobre datasets reducidos y seleccionamos los que daban mejores resultados, y los corrimos sobre el dataset completo para así quedarnos con el mejor modelo.\n",
    "  + Los resultados obtenidos no cumplieron las espectativas (ni en regresion ni en clasificacion) ya que los modelos utilizados en el TP1 dieron mejores métricas\n",
    "  + Creemos que esto se puede deber a varios motivos:\n",
    "    + Mala seleccion de hiperparametros y capas\n",
    "    + Mala adaptacion de las redes a nuestrio problema\n",
    "\n",
    "<br /> \n",
    "\n",
    "+ Parte 3: Ensambles de Modelos\n",
    "  + Para esta parte decidimos hacer 2 modelos para cada ensamble, uno con nuestros anteriores modelos del TP1 (optimizados) y otro con nuevos modelos sin optimizar sus hiperparametros. Si bien las métricas del ensamble de los modelos del TP1 mejoraron con respecto a los modelos sin optimizar, la mejora fue infima. Esto sigue la linea de la frase \"muchos estimadores mediocres juntos generan un buen estimador\".\n",
    "  De ambas formas los modelos obtenidos a travez de los ensambles fueron mejores que los modelos individuales del primer Trabajo Práctico (a excepción de RandomForest, que dió mejores méticas que el ensamble)\n",
    "  \n",
    "  + En conclusión vemos que un modelo de ensamble puede ser muy útil y lograr muy buenas métricas si uno cuenta con modelos poco optimizados, ya que con poco esfuerzo uno puede reutilizarlos y obtener buenos resultados\n",
    "  \n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opciones que hubiesemos explorado y quedaron fuera del alcance de este trabajo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ En cuanto a la primera parte (Ampliación del dataset), se pudo haber usado algun métodos de extracción de conocimiento como lo son ExtrHech, ArgOE, DepOE o ECMes. También se pudieron haber añadido al dataset columnas como cantidad de baños o expensas las cuales tendrían valores extraídos del texto de property_title.\n",
    "\n",
    "+ En cuanto a la segunda parte (Redes Neuronales), se pudo haber probado una grilla con mayor cantidad de hiperparámetros, como por ejemplo reguladores (L1 y L2), early stopping y dropout. Tambien podríamos haber probado otras distintas combinaciones de capas ocultas de neuronas además de probar otro tipo de capas. Se pudo haber probado correr más iteraciones mezclando hiperparámetros.\n",
    "Otra cosa que quedó fuera de nuestro alcance es hacer un ensamble entre nuestros mejores modelos de redes\n",
    "\n",
    "+ En cuanto a la tercera parte podríamos haber usado para los ensambles algunos de nuestros modelos de redes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en los dos modelos planteados, obtenemos que el segundo modelo es el mas preciso. Esto es lo mas lógico ya que es un modelo el cual utiliza modelos de regresión creados especificamente en el TP1, mientras que el otro son modelos creados de cero sin ninguna especificación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b4bfd882b72278d2fc01f7f7f865a6b413108d571dd02a132c40c7c94589e02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
